#!/usr/local/bin/python3.4# -*- coding: utf-8 -*-__author__ = 'wudaown'
##    www.177pic.info/ #   #
import requestsfrom bs4 import BeautifulSoupfrom io import BytesIOimport os

def getSource(url):     #  mapping    r = requests.get(url)    soup = BeautifulSoup(r.text,'lxml')    link = soup.find_all('h2')  # bs4  h2 tag    dl = []    title = [] for x in link:        title.append(x.contents[0]['title'][13:]) # h2 tag tag        dl.append(x.contents[0]['href'])    comic = dict(zip(dl,title)) return(comic)
def getPageNumber(page_url):    #     allPage = []    p = requests.get(page_url)    pagesoup = BeautifulSoup(p.text,'lxml')    page = pagesoup.find(attrs={'class':'wp-pagenavi'}) # attrs if page == None:    # page        number_of_page = 0        allPage.append(page_url) return allPage else:        number_of_page = int(page.contents[0].contents[-3].string)  # page for i in range(number_of_page):            allPage.append(page_url+'/'+str(i+1)) return allPage

def getImglink(page):       #     imgdr = []    p = requests.get(page)    imgsoup = BeautifulSoup(p.text,'lxml')    imglink = imgsoup.findAll('img')    # html for y in imglink: if 'alt' in y.attrs:        #             imgdr.append(y['src']) return  imgdr


def downloadComic(comic_link):      #     imglist = []    comic_page = getPageNumber(comic_link) for x in comic_page:        tmp = getImglink(x) for y in tmp:            imglist.append(y) for z in range(len(imglist)):      # range        img = requests.get(imglist[z-1]) with open(str(z)+'.jpg', 'wb') as f: # wb binary            f.write(img.content)    os.chdir('..')
def getSourcePageNumber():    source = requests.get('http://www.177pic.info/html/category/tt/page/1')    sourcesoup = BeautifulSoup(source.text,'lxml')    sourcepage = sourcesoup.find(attrs={'class':'wp-pagenavi'})    source_page_number = int(sourcepage.contents[-2]['href'].splite('/')[-1]) return source_page_number

def main(): # main  if os.path.exists('recode') == False: print('')        os.popen('touch recode')    #  with open ('recode','w') as f:            f.write('http://www.177pic.info/html/category/tt/page/1') else: print('') with open('recode','r') as f:            trecode = f.readline()  #             recode = trecode.split('/') print('{0}'.format(recode))    url = 'http://www.177pic.info/html/category/tt'    total_page = getSourcePageNumber       url_list = [] for i in range(int(recode[-1]), total_page):    #         url_list.append(url+'/page/'+str(i+1))    tmp = os.popen('ls').readlines()    allcomic = [] for i in tmp:        allcomic.append(i[:-1]) #  del tmp for y in url_list: print(': ',y) with open('recode','w') as f:            f.write(y)        comic = getSource(y) for x in comic: # print(comic[x],end=' ') # print((comic[x]+'.cbr'  in allcomic)) if ((comic[x]+'.cbr') in allcomic) == True: print(comic[x],'') else: print(': ',comic[x]) if (os.path.exists(comic[x])) == True: print('')                    os.chdir(comic[x])                    downloadComic(x)                    command = 'rar a -r -s -m5 -df \''+comic[x]+'.cbr\' \''+comic[x]+'\''                    os.system(command)                    os.system('clear') else:                    os.mkdir(comic[x])                    os.chdir(comic[x])                    downloadComic(x)                    command = 'rar a -r -s -m5 -df \''+comic[x]+'.cbr\' \''+comic[x]+'\''                    os.system(command)                    os.system('clear')
if __name__ == '__main__':    main(url_list)