#!/usr/bin/env python# -*- coding:utf-8 -*-
import sysimport reimport urllib, urllib2import requestsimport pymongoimport datetimeimport multiprocessing as mp

Category_Map = { "1":u"", "2":u"", "3":u"", "4":u"", "5":u"", "9":u"", "10":u"", "11":u"", "12":u"", "13":u"", "14":u"", "15":u"", "16":u"", "17":u""}def num2name(category_num): if Category_Map.has_key(category_num): return Category_Map[category_num] else: return ""
class MongoDBIO: #  def __init__(self, host, port, name, password, database, collection): self.host = host self.port = port self.name = name self.password = password self.database = database self.collection = collection
 # dbposts def Connection(self): # connection = pymongo.Connection() #         connection = pymongo.Connection(host=self.host, port=self.port) # db = connection.datas        db = connection[self.database] if self.name or self.password:            db.authenticate(name=self.name, password=self.password) #  # print "Database:", db.name # posts = db.cn_live_news        posts = db[self.collection] # print "Collection:", posts.name return posts
# # def ResultSave(save_host, save_port, save_name, save_password, save_database, save_collection, save_contents):#     posts = MongoDBIO(save_host, save_port, save_name, save_password, save_database, save_collection).Connection()#     for save_content in save_contents:#         posts.save(save_content)def ResultSave(save_host, save_port, save_name, save_password, save_database, save_collection, save_content):    posts = MongoDBIO(save_host, save_port, save_name, save_password, save_database, save_collection).Connection()    posts.save(save_content)
def Spider(url, data): # # 1requests get    content = requests.get(url=url, params=data).content # GET # # 2urllib2 get # data = urllib.urlencode(data) # dictstring # full_url = url+'?'+data # print full_url # content = urllib2.urlopen(full_url).read() # GET # # content = requests.get(full_url).content # GET # print type(content) # str return content
def ContentSave(item): #     save_host = "localhost"    save_port = 27017    save_name = ""    save_password = ""    save_database = "textclassify"    save_collection = "WallstreetcnSave"
    source = "wallstreetcn"    createdtime = datetime.datetime.now() type = item[0]    content = item[1].decode("unicode_escape") # json'\\uxxxx'unicode_escapeu'\uxxxx'unicode    content = content.encode("utf-8") # print content # district    categorySet = item[2]    category_num = categorySet.split(",")    category_name = map(num2name, category_num)    districtset = set(category_name)&{u"", u"", u"", u"", u"", u"", u"", u"", u""}    district = ",".join(districtset)    propertyset = set(category_name)&{u"", u"", u"", u""} property = ",".join(propertyset)    centralbankset = set(category_name)&{u""}    centralbank = ",".join(centralbankset)    save_content = { "source":source, "createdtime":createdtime, "content":content, "type":type, "district":district, "property":property, "centralbank":centralbank    }    ResultSave(save_host, save_port, save_name, save_password, save_database, save_collection, save_content)
def func(page):    url = "http://api.wallstreetcn.com/v2/livenews" # get    data = { "page":page    }    content = Spider(url, data)    items = re.findall(r'"type":"(.*?)","codeType".*?"contentHtml":"(.*?)","data".*?"categorySet":"(.*?)","hasMore"', content) #  if len(items) == 0: print "The End Page:", page        data = urllib.urlencode(data) # dictstring        full_url = url+'?'+data print full_url        sys.exit(0) #  else: print "The Page:", page, "Downloading..." for item in items:            ContentSave(item)

if __name__ == '__main__':
    start = datetime.datetime.now()
    start_page = 1    end_page = 3300

 #     pages = [i for i in range(start_page, end_page)]    p = mp.Pool()    p.map_async(func, pages)    p.close()    p.join()

 #     page = end_page
 while 1:        url = "http://api.wallstreetcn.com/v2/livenews" # get        data = { "page":page        }        content = Spider(url, data)        items = re.findall(r'"type":"(.*?)","codeType".*?"contentHtml":"(.*?)","data".*?"categorySet":"(.*?)","hasMore"', content) #  if len(items) == 0: print "The End Page:", page            data = urllib.urlencode(data) # dictstring            full_url = url+'?'+data print full_url break else: print "The Page:", page, "Downloading..." for item in items:                ContentSave(item)            page += 1
    end = datetime.datetime.now() print "last time: ", end-start